{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Strips of Stop and Stare Data for Reconstruction\n",
    "This notebook takes a list of existing datasets and performs registration and reconstruction of each stop-and-stare dataset in the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T22:45:49.149388Z",
     "start_time": "2018-10-31T22:45:38.186705Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load motiondeblur module and Dataset class\n",
    "import libwallerlab.projects.motiondeblur as md\n",
    "from libwallerlab.utilities.io import Dataset, isDataset\n",
    "import libwallerlab.utilities.noise as noise\n",
    "\n",
    "# Platform imports\n",
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "# Debugging imports\n",
    "import llops as yp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "yp.config.setDefaultBackend('numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T22:45:50.996107Z",
     "start_time": "2018-10-31T22:45:50.362801Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define user for path setting\n",
    "dataset_path = 'D:\\Zack\\Google Drive\\einstein_data/'\n",
    "\n",
    "# Define output directory\n",
    "output_directory = 'D:\\Zack\\Google Drive\\einstein_data\\patches'\n",
    "\n",
    "# Find files in this directory\n",
    "folder_list = glob.glob(os.path.join(dataset_path, '*/'))\n",
    "dataset_list = [folder for folder in folder_list if isDataset(folder)]\n",
    "\n",
    "# Select only the stop and stare datasets\n",
    "sns_dataset_list = [folder_name for folder_name in folder_list if 'stopandstare' in folder_name]\n",
    "coded_dataset_list = [folder_name for folder_name in folder_list if 'coded' in folder_name]\n",
    "\n",
    "# Select res target (for debugging)\n",
    "sns_dataset_list = [s for s in sns_dataset_list if '173e' in s]\n",
    "# coded_dataset_list = [s for s in coded_dataset_list if 'res' in s]\n",
    "\n",
    "# Select dataset (for now)\n",
    "dataset_index = 0\n",
    "\n",
    "# Create dataset object for stop and stare\n",
    "dataset = Dataset(sns_dataset_list[dataset_index])\n",
    "dataset.metadata.type = 'motiondeblur'\n",
    "dataset.channel_mask = [0]\n",
    "md.preprocess(dataset)\n",
    "\n",
    "# # Create dataset for coded illumination\n",
    "# dataset_coded = Dataset(coded_dataset_list[dataset_index])\n",
    "# dataset_coded.metadata.type = 'motiondeblur'\n",
    "# dataset_coded.channel_mask = [0]\n",
    "# md.preprocess(dataset_coded)\n",
    "\n",
    "# Get linear segment count\n",
    "linear_segment_count = len(dataset.position_segment_indicies)\n",
    "# linear_segment_count_coded = len(dataset_coded.position_segment_indicies)\n",
    "# assert linear_segment_count_coded == linear_segment_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Step: Generate Raw Data and Corresponding Blurred Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take full unblurred measurement\n",
    "2. Decimate\n",
    "3. Convolve with decimated blur kernel (return valid kernel)\n",
    "4. Crop decimated ground truth to \n",
    "\n",
    "Kernel offset to left should have true values on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-31T22:45:57.523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear all frames from memory\n",
    "dataset._frame_list# = [None] * len(dataset._frame_list)\n",
    "\n",
    "# Set frame mask\n",
    "# dataset.frame_mask = [frame_index]\n",
    "\n",
    "# Load frame\n",
    "# frame = dataset.frame_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-31T22:41:46.435Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import libwallerlab.utilities.noise as noise\n",
    "downsample_factor = 8\n",
    "compress_output = True\n",
    "blur_kernel_fov_fraction = 0.2\n",
    "frame_overlap_fraction = 0.25\n",
    "blur_axis = 1\n",
    "blur_direction = 'left'\n",
    "debug=True\n",
    "\n",
    "# Define noise models. Dict items are kwargs for libwallerlab.utilities.noise.add()\n",
    "noise_models = {'gaussian': {'snr': 255}}\n",
    "\n",
    "# Get frame shape\n",
    "frame_shape = [sp.fftpack.next_fast_len(int(sz / downsample_factor)) for sz in dataset.frame_shape]\n",
    "\n",
    "# Get measurement shap3\n",
    "blur_vector_length = int((blur_kernel_fov_fraction * frame_shape[blur_axis]))\n",
    "\n",
    "# Determine primary measurement shape\n",
    "measurement_shape = yp.dcopy(frame_shape)\n",
    "measurement_shape[blur_axis] = frame_shape[blur_axis] - blur_vector_length + 1\n",
    "measurement_start = [0, 0]\n",
    "\n",
    "# Determine shape of overlap\n",
    "overlap_shape = yp.dcopy(frame_shape)\n",
    "overlap_shape[blur_axis] = int(frame_shape[blur_axis] * frame_overlap_fraction)\n",
    "overlap_start = [0, 0]\n",
    "overlap_start[blur_axis] = frame_shape[blur_axis] - overlap_shape[blur_axis] \n",
    "\n",
    "# Get ground truth shape\n",
    "ground_truth_shape = yp.dcopy(frame_shape)\n",
    "ground_truth_shape[blur_axis] = measurement_shape[blur_axis] - overlap_shape[blur_axis]\n",
    "ground_truth_start = [0, 0]\n",
    "\n",
    "if debug:\n",
    "    print('Frame shape is %s' % str(frame_shape))\n",
    "    print('Blur vector length is %d' % blur_vector_length)\n",
    "    print('Measurement shape is %s' % str(measurement_shape))\n",
    "    print('Overlap shape is %s' % str(overlap_shape))\n",
    "    print('Ground Truth shape is %s' % str(ground_truth_shape))\n",
    "\n",
    "# Calculate size of first (left) frame\n",
    "frame_1_shape = measurement_shape\n",
    "frame_1_start = measurement_start\n",
    "\n",
    "# Calculate size of first (left) frame\n",
    "frame_2_shape = overlap_shape\n",
    "frame_2_start = overlap_start\n",
    "\n",
    "# Generate blurring function and decimate\n",
    "blur_kernel = md.blurkernel.generate(frame_shape, blur_vector_length, axis=blur_axis, \n",
    "                                     position='center_' + blur_direction, method='random_phase')\n",
    "\n",
    "# Also crop blur kernel for storage in output file\n",
    "blur_kernel_crop = yp.crop_to_support(blur_kernel)\n",
    "\n",
    "# Loop over measurements and generate datapoints\n",
    "for frame_index in yp.display.progressBar(range(len(dataset.frame_mask)), name='Frames Saved'):\n",
    "    # Clear all frames from memory\n",
    "    dataset._frame_list = [None] * len(dataset._frame_list)\n",
    "    \n",
    "    # Set frame mask\n",
    "    dataset.frame_mask = [frame_index]\n",
    "    \n",
    "    # Load frame\n",
    "    frame = dataset.frame_list[0]\n",
    "    \n",
    "    # Decimite frame\n",
    "    frame_decimated = yp.filter.decimate(frame, downsample_factor)\n",
    "\n",
    "    # Convolve with blurring function ('valid' keyword crops kernel to )\n",
    "    frame_convolved = yp.convolve(frame_decimated, blur_kernel, mode='same', padded=False)\n",
    "\n",
    "    # Crop first frame's roi\n",
    "    frame_1 = yp.crop(frame_convolved, frame_1_shape, frame_1_start)\n",
    "    \n",
    "    # Crop to second frame's ROI\n",
    "    frame_2 = yp.crop(frame_convolved, frame_2_shape, frame_2_start)\n",
    "    \n",
    "    # Add noise to measurements\n",
    "    for noise_mode in noise_models:\n",
    "        frame_1 = noise.add(frame_1, noise_mode, **noise_models[noise_mode])\n",
    "        frame_2 = noise.add(frame_2, noise_mode, **noise_models[noise_mode])\n",
    "        \n",
    "    # Generate ground truth image with correct support\n",
    "    ground_truth = yp.crop(frame_decimated, ground_truth_shape, ground_truth_start)\n",
    "    \n",
    "    # Generate output filename\n",
    "    _dir = os.path.join(output_directory, dataset.metadata.file_header) + '_%d' % frame_index\n",
    "    \n",
    "    # Define data structure\n",
    "    data = {'ground_truth': {'array': ground_truth, 'start': (0,0)},\n",
    "            'measurements': [{'array': frame_1, 'start': frame_1_start},\n",
    "                             {'array': frame_2, 'start': frame_2_start}],\n",
    "            'metadata': {'blur_direction': blur_direction, \n",
    "                         'blur_axis': blur_axis,\n",
    "                         'blur_kernel_fov_fraction': blur_kernel_fov_fraction,\n",
    "                         'frame_overlap_fraction': frame_overlap_fraction,\n",
    "                         'measurement_shape': measurement_shape,\n",
    "                         'ground_truth_shape': ground_truth_shape},\n",
    "            'blur_vector': {'array': blur_kernel_crop, 'start': yp.boundingBox(blur_kernel)[0]}}\n",
    "    \n",
    "    # Save data\n",
    "    if compress_output:\n",
    "        np.savez_compressed(_dir, data)\n",
    "    else:\n",
    "        np.savez(_dir, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Display a Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T21:46:41.974048Z",
     "start_time": "2018-10-31T21:46:41.888652Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set data point index here\n",
    "frame_index = 6\n",
    "\n",
    "# Find frames\n",
    "files = list(glob.glob(output_directory + '*.npz'))\n",
    "files.sort()\n",
    "\n",
    "# Load data point (second line deals with weird structuring of .npz files)\n",
    "_data = dict(np.load(files[frame_index]))\n",
    "data = {key:_data[key].item() for key in _data}['arr_0']\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(141)\n",
    "plt.imshow(yp.real(data['measurements'][0]['array']))\n",
    "plt.title('frame 1')\n",
    "plt.subplot(142)\n",
    "plt.imshow(yp.real(data['measurements'][1]['array']))\n",
    "plt.title('frame 2')\n",
    "plt.subplot(143)\n",
    "plt.imshow(yp.real(data['ground_truth']['array']))\n",
    "plt.title('ground truth')\n",
    "plt.subplot(144)\n",
    "plt.plot(yp.real(np.squeeze(data['blur_vector']['array'])))\n",
    "plt.title('Blur Sequence')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconvolve a Data Point Using L2 Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T21:46:44.605146Z",
     "start_time": "2018-10-31T21:46:44.512917Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set data point index here\n",
    "frame_index = 4\n",
    "\n",
    "# Find frames\n",
    "files = list(glob.glob(output_directory + '*.npz'))\n",
    "files.sort()\n",
    "\n",
    "# Load data point (second line deals with weird structuring of .npz files)\n",
    "_data = dict(np.load(files[frame_index]))\n",
    "data = {key:_data[key].item() for key in _data}['arr_0']\n",
    "\n",
    "blur_vector = data['blur_vector']['array']\n",
    "measurement_shape = data['metadata']['measurement_shape']\n",
    "ground_truth_shape = data['metadata']['ground_truth_shape']\n",
    "blur_axis = data['metadata']['blur_axis']\n",
    "blur_direction = data['metadata']['blur_direction']\n",
    "\n",
    "frame_1 = data['measurements'][0]['array']\n",
    "frame_2 = data['measurements'][1]['array']\n",
    "ground_truth = data['ground_truth']['array']\n",
    "\n",
    "# Get ROIs\n",
    "frame_1_roi = yp.Roi(start=data['measurements'][0]['start'], shape=yp.shape(frame_1))\n",
    "frame_2_roi = yp.Roi(start=data['measurements'][1]['start'], shape=yp.shape(frame_2))\n",
    "\n",
    "# Average measurements\n",
    "coverage_weights = yp.zeros(measurement_shape)\n",
    "coverage_weights[frame_1_roi.slice] += 1.0\n",
    "coverage_weights[frame_2_roi.slice] += 1.0\n",
    "measurements = (yp.pad(frame_1, measurement_shape, frame_1_roi.start) + yp.pad(frame_2, measurement_shape, frame_2_roi.start)) / coverage_weights\n",
    "\n",
    "# Create blur kernel with the correct position in the frame\n",
    "# blur_kernel_crop = yp.roll(md.blurkernel.fromVector(blur_vector, measurement_shape, axis=blur_axis, position='center_' + blur_direction), (-1, 2))\n",
    "import math\n",
    "blur_kernel_crop = yp.roll(yp.pad(blur_vector, measurement_shape, center=True), (0, -math.ceil(yp.size(blur_vector) / 2) + 3))\n",
    "# Generate operators\n",
    "import ndoperators as ops\n",
    "CR = ops.Crop(measurement_shape, ground_truth_shape, crop_start=(0,0))\n",
    "C = ops.Convolution(blur_kernel_crop, dtype='complex32', mode='circular')\n",
    "y = measurements\n",
    "\n",
    "# Define deconvolution method\n",
    "method = 'direct'\n",
    "\n",
    "if method == 'gd':\n",
    "    objective = ops.solvers.objectivefunctions.L2(C, y)\n",
    "    gd = ops.solvers.GradientDescent(objective)\n",
    "    x_opt = CR * gd.solve(iteration_count=1000, step_size=1e-3)\n",
    "    \n",
    "elif method == 'direct':\n",
    "    C.inverse_regularizer = 1e-1\n",
    "    x_opt = CR * C.inv * y\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(141)\n",
    "plt.imshow(yp.abs(frame_1))\n",
    "plt.title('Raw')\n",
    "plt.subplot(142)\n",
    "plt.imshow(yp.abs(x_opt))\n",
    "plt.title('Recovered')\n",
    "plt.subplot(143)\n",
    "plt.imshow(yp.abs(ground_truth))\n",
    "plt.title('True')\n",
    "plt.subplot(144)\n",
    "plt.imshow(yp.abs(ground_truth - x_opt))\n",
    "plt.title('Error')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "148px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "807px",
    "left": "2098px",
    "right": "20px",
    "top": "144px",
    "width": "341px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
